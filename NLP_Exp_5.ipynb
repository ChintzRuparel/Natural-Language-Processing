{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChintzRuparel/Natural-Language-Processing/blob/main/NLP_Exp_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries"
      ],
      "metadata": {
        "id": "IAnAoZ2E6Tbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Name: Chintan Ruparel\n",
        "\n",
        "Roll no. C086\n",
        "\n",
        "Division: C\n",
        "\n",
        "Batch: C1"
      ],
      "metadata": {
        "id": "g0gn2yQVyn_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import multiprocessing\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "b10kZo4q2Odf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Obtain page content"
      ],
      "metadata": {
        "id": "7YMlDdrM3gHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Lionel_Messi\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "#first_paragraph = soup.find(\"p\",)\n",
        "first=soup.find_all('p')\n",
        "data1=first[7].text\n",
        "# text=first_paragraph.get_text()\n",
        "# print(soup)\n",
        "# Convert the data to a pandas dataframe\n",
        "df = pd.DataFrame(data1.split(\" \"),columns=['words'])\n",
        "\n",
        "\n",
        "# data1 = .find_all('div', {'class': 'eachStory'})\n",
        "data2, data3, data4 = [], [], []\n",
        "\n",
        "work = ''\n",
        "# for d in data1:\n",
        "  # data2.append(d.find_all('h3'))\n",
        "  # data4.append(d.find_all('p'))\n",
        "# for d in data2:\n",
        "  # data3.append(d[0].find_all('a'))\n",
        "\n",
        "# for w in range(0,len(data4)):\n",
        "    # work = work+data4[1][0].text\n",
        "# df = pd.DataFrame(work.split(\" \"),columns=['words'])\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "bow = vectorizer.fit_transform(df['words'])\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(feature_names)\n",
        "\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")\n",
        "\n",
        "\n",
        "model1 = Word2Vec(feature_names, min_count = 1,size = 100, window = 5)\n",
        "\n",
        "model2 = Word2Vec(feature_names, min_count = 1, size = 100, window = 5, sg = 1)\n",
        "\n",
        "# model2.mw.similarity('goals','messi')\n",
        "model2.wv.most_similar('goals','messi')\n",
        "\n",
        "import gensim.downloader\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "glove_vectors.most_similar('twitter')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVzeSylv3mrS",
        "outputId": "b755e426-e119-4705-ad17-7aa42d722a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10' '17' '192' '1987' '2020' '2021' '2022' '24' '35' '36' '474' '672'\n",
            " '795' '98' 'all' 'also' 'american' 'américa' 'an' 'and' 'andrés'\n",
            " 'anˈdɾes' 'argentina' 'argentine' 'as' 'assists' 'awards' 'ballon'\n",
            " 'barcelona' 'born' 'by' 'captains' 'career' 'champions' 'club' 'copa'\n",
            " 'country' 'creative' 'cup' 'del' 'dream' 'entire' 'european' 'fifa'\n",
            " 'footballer' 'for' 'forward' 'four' 'germain' 'goals' 'goalscorer'\n",
            " 'golden' 'greatest' 'had' 'has' 'hat' 'he' 'his' 'holds' 'in' 'including'\n",
            " 'international' 'is' 'june' 'known' 'la' 'league' 'leagues' 'leaving'\n",
            " 'leo' 'liga' 'ligue' 'lionel' 'listen' 'ljoˈnel' 'male' 'messi' 'most'\n",
            " 'named' 'national' 'note' 'of' 'one' 'or' 'over' 'paris' 'player'\n",
            " 'players' 'playmaker' 'plays' 'professional' 'prolific' 'pronunciation'\n",
            " 'record' 'records' 'regarded' 'rey' 'saint' 'scored' 'senior' 'seven'\n",
            " 'shoes' 'single' 'six' 'south' 'spanish' 'spent' 'team' 'the' 'time'\n",
            " 'titles' 'to' 'tricks' 'trophies' 'uefa' 'until' 'was' 'where' 'who'\n",
            " 'widely' 'with' 'won' 'world' 'ˈmesi']\n",
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('facebook', 0.9480051398277283),\n",
              " ('tweet', 0.9403422474861145),\n",
              " ('fb', 0.9342358708381653),\n",
              " ('instagram', 0.9104823470115662),\n",
              " ('chat', 0.8964964747428894),\n",
              " ('hashtag', 0.8885936141014099),\n",
              " ('tweets', 0.8878157734870911),\n",
              " ('tl', 0.8778461813926697),\n",
              " ('link', 0.877821147441864),\n",
              " ('internet', 0.8753897547721863)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Questions of Curiosity**\n",
        "#Question 1: What are basic ways of word representation in NLP?\n",
        "Ans 1:- The most common ways of representing words in NLP are:\n",
        "\n",
        "One-Hot Encoding: where each word is represented as a sparse vector of all zeros with a 1 at the index of the word in the vocabulary.\n",
        "\n",
        "Word Embeddings: where words are mapped to dense continuous vectors, capturing semantic and syntactic similarities between words. The most commonly used word embeddings are word2vec and GloVe.\n",
        "\n",
        "Character Embeddings: where words are represented based on their constituent characters.\n",
        "\n",
        "BERT Embeddings: where each token is represented by a combination of sub-word, position, and context-based embeddings generated by the BERT language model.\n",
        "\n",
        "Hybrid Representations: which combine multiple types of word representations to leverage their strengths.\n",
        "\n",
        "#Question 2: What is the importance of pre-trained word embeddings?\n",
        "Pre-trained word embeddings are important in NLP for several reasons:\n",
        "\n",
        "Transfer Learning: Pre-trained word embeddings can be used as a starting point for fine-tuning on specific NLP tasks, allowing models to leverage the knowledge gained from large amounts of generic text data.\n",
        "\n",
        "Better Representation: Pre-trained word embeddings capture the semantic and syntactic relationships between words, providing a dense and meaningful representation compared to one-hot encoding.\n",
        "\n",
        "Reduced Training Time: Using pre-trained word embeddings can significantly reduce the amount of training time required, as the embeddings have already been learned from a large corpus of text.\n",
        "\n",
        "Improved Performance: Pre-trained word embeddings have been shown to improve the performance of NLP models on a variety of tasks, such as text classification and named entity recognition, compared to models trained from scratch.\n",
        "\n",
        "#Question 3: Explain popular types of pretrained word embeddings – Word2Vec and GloVe.\n",
        "\n",
        "Word2Vec and GloVe are two of the most popular pretrained word embeddings.\n",
        "\n",
        "Word2Vec is a two-layer neural network-based model developed by Google in 2013. It uses a continuous bag of words (CBOW) or skip-gram architecture to represent words as dense vectors in a low-dimensional space. These vectors capture the context in which words appear and their relationships with other words. The idea behind Word2Vec is that words that have similar contexts will have similar vector representations.\n",
        "\n",
        "GloVe (Global Vectors for Word Representation) is a word embedding method developed by Stanford University in 2014. GloVe is based on the co-occurrence matrix of words, where the matrix entries represent the number of times two words appear together in a corpus. The co-occurrence matrix is factorized into two lower-dimensional matrices, which are used to represent words as vectors. The vectors capture the global co-occurrence information of words.\n",
        "\n",
        "#Question 4: 4.\tCompare and contrast pretrained word embeddings and learning embeddings from scratch\n",
        "\n",
        "Pretrained word embeddings and learning embeddings from scratch are two different approaches to generate word representations in NLP.\n",
        "\n",
        "Pretrained word embeddings are pre-computed and pre-trained on a large corpus, and then used as the initial word representation in another NLP task. They are widely used because they can save time and computation compared to learning the word representations from scratch. Pretrained word embeddings can provide a good starting point for NLP tasks, especially when the training data is limited or noisy. They also often provide a better representation of words than starting from random vectors, as they are learned from a large corpus.\n",
        "\n",
        "Learning embeddings from scratch means training the word representations from scratch on the task-specific data. This method is appropriate when the training data is large enough to learn good representations and when the pretrained embeddings are not suitable for the specific NLP task. Learning embeddings from scratch also allows the embeddings to be fine-tuned to the task-specific data, which can lead to improved performance.\n"
      ],
      "metadata": {
        "id": "aqBmUSZa4MIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusion**\n",
        "Learning vector space models and Word embedding"
      ],
      "metadata": {
        "id": "sErfDy7w6a95"
      }
    }
  ]
}